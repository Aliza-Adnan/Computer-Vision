{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6UqN4vB_vfo",
        "outputId": "adc123de-554e-4dd2-ae9c-d921176ee040"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdaiBD4sCRwF",
        "outputId": "14a18a71-c90e-49d5-bdc3-c32b0c834e1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "tTQPqaOLbIdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "#making class of 2 convolution layers in a step\n",
        "class DoubleConv(nn.Module):                                             \n",
        "\tdef __init__(self,in_c,out_c):\n",
        "\t\tsuper(DoubleConv,self).__init__()\n",
        "\t\tself.conv = nn.Sequential(# as it is not random so we are using sequential\n",
        "\t\t\tnn.Conv2d(in_c,out_c,3,1,1,bias=False),# kernel size=3, stride and padding=1 according to the paper \n",
        "\t\t\tnn.BatchNorm2d(out_c),\n",
        "\t\t\tnn.ReLU(inplace=True),#and bias is False as we are usng batch norm so if we use both it cancels out each others effect\n",
        "\t\t\tnn.Conv2d(out_c,out_c,3,1,1,bias=False),# here we are using the same input and output i.e out \n",
        "\t\t\tnn.BatchNorm2d(out_c),\n",
        "\t\t\tnn.ReLU(inplace=True))#channels because the depth remains same in one step\n",
        "\n",
        "\tdef forward(self,a):# this is doing forward pass\n",
        "\t\treturn self.conv(a)\n",
        "\n",
        "\n",
        " # this is the UNET architecture implementation\n",
        "class UNet(nn.Module):\n",
        "\tdef __init__(self,in_c = 3,out_c = 1,features=[64,128,256,512]):# as RGB image is input so we are taking 3 channels,output channels have 1 as we are doing binary segmentation\n",
        " # features represent the depth size that is increasing or decreasing per step after 2 convolutions.\n",
        "\t\tsuper(UNet,self).__init__()\n",
        "\t\tself.up=nn.ModuleList()# the features/depth list storing the information of input and output at upsampling process\n",
        "\t\tself.down = nn.ModuleList()# the features/depth list storing the information of input and output at downsampling process\n",
        "\t\tself.pool = nn.MaxPool2d(kernel_size=2,stride=2)# application of maxpooling after each step(application of 2 convolution layers)\n",
        "\n",
        "\t\t#going down\n",
        "\t\tfor f in features:\n",
        "\t\t\tself.down.append(DoubleConv(in_c,f))\n",
        "\t\t\tin_c = f\n",
        "\n",
        "\t\t#bottom layer\n",
        "\t\tself.neck = DoubleConv(features[-1],features[-1]*2) # the bottleneck or the in between layer between upsampling and downsampling\n",
        "\t\t\n",
        "\n",
        "\t\t#going up \n",
        "\t\tfor feature in reversed(features):\n",
        "\t\t\tself.up.append(nn.ConvTranspose2d(feature*2 , feature, kernel_size =2,stride=2,))\n",
        "\t\t\tself.up.append(DoubleConv(feature*2,feature))\n",
        "\n",
        "\t\t\n",
        "\t\tself.final_conv = nn.Conv2d(features[0],out_c,kernel_size=1)#the final layer after upsampling\n",
        "\n",
        "\n",
        "\tdef forward(self,a):\n",
        "\t\tskip_connections = []\n",
        "\n",
        "\t\tfor layers in self.down:#applying the downsamping and the 2 convolution layer step\n",
        "\t\t\ta = layers(a)\n",
        "\t\t\tskip_connections.append(a)\n",
        "\t\t\ta = self.pool(a)\n",
        "\n",
        "\n",
        "\t\ta = self.neck(a)#applying the bottleneck\n",
        "\n",
        "\t\tskip_connections = skip_connections[::-1] #reverse the list #skip connection for upsampling\n",
        "\t\t#skip_connections = skip_connections.reverse()\n",
        "\n",
        "\t\tfor idx in range(0,len(self.up),2):#applying downsampling steps\n",
        "\t\t\ta = self.up[idx](a)\n",
        "\t\t\tskip_connection = skip_connections[idx //2]\n",
        "\t\t\t\n",
        "\t\t\tif a.shape != skip_connection.shape:#for equal size of input and output image\n",
        "\t\t\t\ta = TF.resize(a,size=skip_connection.shape[2:])  #resize ,[2:] get the current shape\n",
        "\n",
        "\t\t\tconcat_skip  = torch.cat((skip_connection,a),dim=1)\n",
        "\n",
        "\t\t\ta = self.up[idx+1](concat_skip)\n",
        "\n",
        "\n",
        "\t\ta = self.final_conv(a)#applying the final layer convolution after upsampling\n",
        "\n",
        "\t\treturn a\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "\ta = torch.rand((3,1,161,161))\n",
        "\tmodel = UNet(1,1)\n",
        "\tpred = model(a)\n",
        "#\tmodel.save('/content/drive/MyDrive/UNET/') \n",
        "\tprint(pred.shape)\n",
        "\tassert pred.shape==a.shape\n",
        "\n",
        "#if __name__ =='__main__':\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM8g_tAgRS96",
        "outputId": "174b62fb-86d7-4655-c00b-8c9be3f4b688"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 161, 161])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "7_HAn0KtbZow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "class data(Dataset):\n",
        "  def __init__(self,image_dir,mask_dir,transform=None):# assigning respective directories\n",
        "    self.image_dir=image_dir\n",
        "    self.mask_dir=mask_dir\n",
        "    self.transform=transform\n",
        "    self.images=os.listdir(image_dir)#listing all those files in that folder\n",
        "  def __len__(self):#getting the length of the images\n",
        "    return len(self.images)\n",
        "  def __getitem__(self,index):\n",
        "    img_path=os.path.join(self.image_dir,self.images[index])\n",
        "    mask_path=os.path.join(self.mask_dir,self.images[index])\n",
        "    image=np.array(Image.open(img_path).convert(\"RGB\"))#converting to colored image\n",
        "    mask=np.array(Image.open(mask_path).convert(\"L\"),dtype=np.float32)#converting to binary image\n",
        "    mask[mask==255.0]=1.0\n",
        "    if self.transform is not None:#data augmentation\n",
        "      augmentations=self.transform(image=image,mask=mask)\n",
        "      image=augmentations[\"image\"]\n",
        "      mask=augmentations[\"mask\"]\n",
        "    return image,mask\n"
      ],
      "metadata": {
        "id": "4UZUHO1fKRFM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "MaI6ZRZEbWgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "device = 'cuda' #if torch.cuda.is_available() else'cpu'\n",
        "#saving checkpoint\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "#loading that checkpoint\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,#makes sure that no copy elements are on the disk\n",
        "):\n",
        "#training dataset\n",
        "    train_ds = data(\n",
        "        image_dir=train_dir,\n",
        "        mask_dir=train_maskdir,\n",
        "        transform=train_transform,\n",
        "    )\n",
        "#training loader\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True,\n",
        "    )\n",
        "#validation dataset\n",
        "    val_ds = data(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "#validation loader\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():#checking against pixels accuracy\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)#mounting to gpu\n",
        "            y = y.to(device).unsqueeze(1)#mounting to gpu\n",
        "            preds = torch.sigmoid(model(x))#applying the sigmoid function\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()#accumulating the predictions\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / (\n",
        "                (preds + y).sum() + 1e-8\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
        "    )\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def save_predictions_as_imgs(#saving the predictions\n",
        "    loader, model, folder=\"saved_images/\", device=\"cuda\"\n",
        "):\n",
        "    model.eval()#evaluating\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "        torchvision.utils.save_image(\n",
        "            preds, f\"{folder}/pred_{idx}.png\"\n",
        "        )\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
        "\n",
        "  #  model.train()"
      ],
      "metadata": {
        "id": "wYzKKeJiSrJQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with test"
      ],
      "metadata": {
        "id": "dqiSlrYywb2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else'cpu'\n",
        "batch_size = 32\n",
        "num_epochs = 1\n",
        "num_workers = 2\n",
        "image_height = 160  #1280 og\n",
        "image_width = 240 #1918 og\n",
        "pin_memory = True \n",
        "load_model = False\n",
        "train_img_dir = '/content/drive/MyDrive/dataset/data/train/image/'\n",
        "train_mask_dir = '/content/drive/MyDrive/dataset/data/train/mask/'\n",
        "val_img_dir = '/content/drive/MyDrive/dataset/data/test/image/'\n",
        "val_mask_dir='/content/drive/MyDrive/dataset/data/test/mask/'\n",
        "\n",
        "def train_fn(loader,model,optimizer,loss_fn,scaler):\n",
        "\tloop = tqdm(loader)#making progress bar\n",
        "\n",
        "\tfor batch_idx,(data,targets) in enumerate(loop):\n",
        "\t\tdata = data.to(device)\n",
        "\t\ttargets = targets.float().unsqueeze(1).to(device=device)\n",
        "\n",
        "\t\twith torch.cuda.amp.autocast():#pushig data to 16bit\n",
        "\t\t\tpredictions = model(data)\n",
        "\t\t\tloss = loss_fn(predictions,targets)\n",
        "\n",
        "\t\toptimizer.zero_grad()#backward propagation\n",
        "\t\tscaler.scale(loss).backward()\n",
        "\t\tscaler.step(optimizer)\n",
        "\t\tscaler.update()\n",
        "\n",
        "\n",
        "\n",
        "\t\t#tqdmloop\n",
        "\t\tloop.set_postfix(loss=loss.item())#showing loss function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data augmentation main part\n",
        "train_transform = A.Compose([ \n",
        "\t\tA.Resize(height=image_height,width = image_width),\n",
        "\t\tA.Rotate(limit=35,p=1.0),\n",
        "\t\tA.HorizontalFlip(p=0.5),\n",
        "\t\tA.VerticalFlip(p=0.1),\n",
        "\t\tA.Normalize(\n",
        "\t\t\tmean=[0.0,0.0,0.0],\n",
        "\t\t\tstd=[1.0,1.0,1.0],\n",
        "\t\t\tmax_pixel_value = 255.0\n",
        "\t\t\t),\n",
        "\t\tToTensorV2(), ] )\n",
        "val_transforms = A.Compose(\n",
        "\n",
        "\t\t[ \n",
        "\n",
        "\t\tA.Resize(height=image_height,width = image_width),\n",
        "\t\tA.Normalize(\n",
        "\t\t\tmean=[0.0,0.0,0.0],\n",
        "\t\t\tstd=[1.0,1.0,1.0],\n",
        "\t\t\tmax_pixel_value = 255.0\n",
        "\t\t\t),\n",
        "\t\tToTensorV2(), ] )\n",
        "\n",
        "model = UNet(3,1).to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()#binary cross entropy loss for binary image\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)#optimizer function\n",
        "\n",
        "\n",
        "train_loader,val_loader = get_loaders(\n",
        "\t\ttrain_img_dir,\n",
        "\t\ttrain_mask_dir,\n",
        "\t\tval_img_dir,\n",
        "\t\tval_mask_dir,\n",
        "\t\tbatch_size,\n",
        "\t\ttrain_transform,\n",
        "    val_transforms,\n",
        "\t\tnum_workers,\n",
        "\t\tpin_memory,\n",
        "\t)\n",
        "scaler = torch.cuda.amp.GradScaler()#gradient scaling\n",
        "for epoch in range(num_workers):\n",
        "\ttrain_fn(train_loader,model,optimizer,loss_fn,scaler)\n",
        "\n",
        "\t\t#save\n",
        "\t\t#check accuracy\n",
        "\tcheckpoint = {\n",
        "\t\t\t'state_dic':model.state_dict(),\n",
        "\t\t\t'optimizer':optimizer.state_dict(),\n",
        "\t\t}\n",
        "\tsave_checkpoint(checkpoint)\n",
        "\n",
        "\tcheck_accuracy(val_loader,model,device=device)\n",
        "\n",
        "\tsave_predictions_as_imgs(\n",
        "\t\tval_loader,model,folder='/content/drive/MyDrive/dataset/data/saved_images/',device=device)\n",
        "\n",
        "\n",
        "\n",
        "#main()#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVpoQOAPR2Is",
        "outputId": "235c8ebc-7f5f-4bc5-dc5c-4ce36d7bcdaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 69/69 [05:27<00:00,  4.75s/it, loss=0.614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Got 14884255/21043200 with acc 70.73\n",
            "Dice score: 9.126721124630421e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.672]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "Got 14884799/21043200 with acc 70.73\n",
            "Dice score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most difficult part was the integration and how the steps were broken down as we have never implemented a neuralnetwork of this size. By watching the tutorial, i was able to grasp the understanding of the integration and connection."
      ],
      "metadata": {
        "id": "jhWBC7G_tMmZ"
      }
    }
  ]
}